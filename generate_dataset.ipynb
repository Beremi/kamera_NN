{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from kamera import kamera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(1e4)\n",
    "bounds = 3 * np.array([50 * 1e-3, 50 * 1e-3, 50 * 1e-3, 20 * np.pi / 180, 20 * np.pi / 180, 20 * np.pi / 180])\n",
    "output_data = bounds * (np.random.rand(n, 6) - 0.5) * 2\n",
    "input_data = kamera(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rozdělit data na trénovací a testovací set\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_data, output_data, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 512)\n",
    "        self.fc3 = nn.Linear(512, 512)\n",
    "        self.fc4 = nn.Linear(512, 512)\n",
    "        #self.fc5 = nn.Linear(16, 16)\n",
    "        #self.fc6 = nn.Linear(16, 16)\n",
    "        #self.fc7 = nn.Linear(16, 16)\n",
    "        self.fc8 = nn.Linear(512, output_size)\n",
    "        self.relu = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu(self.fc4(x))\n",
    "        #x = self.relu(self.fc5(x))\n",
    "        #x = self.relu(self.fc6(x))\n",
    "        #x = self.relu(self.fc7(x))\n",
    "        x = self.fc8(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Assuming X_train, X_test, y_train are already defined somewhere in your code\n",
    "# Create the model and transfer it to the GPU\n",
    "model = Net(X_train.shape[1], y_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "# Load the model weights from HDF5 file\n",
    "with h5py.File('model_weights13.h5', 'r') as h5file:\n",
    "    for name, param in model.named_parameters():\n",
    "        # Ensure the parameter name matches the HDF5 dataset name structure\n",
    "        param.data.copy_(torch.from_numpy(h5file[name][...]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = np.array([0.0028, 0.0015, 0.0094, 0.0083, 0.0092, \n",
    "                0.0074, 0.0089, 0.0072, 0.0089, 0.0083])\n",
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial_sets in range(100000):\n",
    "\n",
    "    n = int(1e5)\n",
    "    bounds = 1.05*np.array([50 * 1e-3, 50 * 1e-3, 50 * 1e-3, 20 * np.pi / 180, 20 * np.pi / 180, 20 * np.pi / 180])\n",
    "    output_data = bounds * (np.random.rand(n, 6) - 0.5) * 2\n",
    "    input_data = kamera(output_data) + np.random.normal(scale=std, size=(n, 10))*0\n",
    "    # rozdělit data na trénovací a testovací set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(input_data, output_data, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Convert data to PyTorch tensors and send to the device (GPU if available)\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Define the loss function and the optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.000002)  # Decrease the learning rate\n",
    "\n",
    "    # Add a learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=1000, factor=0.5)\n",
    "\n",
    "    # Assuming all previous setup code remains the same and tensors (X_train_tensor, y_train_tensor, X_test_tensor)\n",
    "    # are already transferred to the appropriate device (GPU or CPU)\n",
    "\n",
    "    # No need for DataLoader as we will not use mini-batches\n",
    "\n",
    "    # Train the model without using batches\n",
    "    num_epochs = 1000\n",
    "    best_loss = np.inf\n",
    "    patience, trials = 2000, 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Ensure the entire dataset is on the same device as the model\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Adjust the learning rate based on the loss\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            outputs = model(X_test_tensor)\n",
    "            loss2 = criterion(outputs, y_test_tensor)\n",
    "            print('Epoch [{}/{}], Loss: {:.10f}, test Loss reached {:.10f}, min Loss reached {:.10f}, stagnating for {} it.'.format(epoch +\n",
    "                1, num_epochs, loss.item(),loss2.item(), best_loss, trials))\n",
    "            if loss2.item() > 10*best_loss:\n",
    "                print(f\"Overfitting detected at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # Early stopping\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            trials = 0\n",
    "        else:\n",
    "            trials += 1\n",
    "            if trials >= patience:\n",
    "                print(f'Stopping early at epoch {epoch+1}')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_lr = optimizer.param_groups[0]['lr']\n",
    "print(f\"Current learning rate: {current_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_tensor = model(X_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_surr = y_pred_tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = y_test - res_surr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.abs(diff)[:,1], bins=100)\n",
    "np.max(np.abs(diff), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_tensor = model(X_train_tensor)\n",
    "res_surr = y_pred_tensor.cpu().numpy()\n",
    "diff = y_train - res_surr\n",
    "np.max(np.abs(diff).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# If your model is on CUDA, move it back to CPU\n",
    "model.to('cpu')\n",
    "\n",
    "# Assuming the model is trained here and ready to be saved\n",
    "\n",
    "# Save model parameters to HDF5, including biases\n",
    "with h5py.File('model_weights15.h5', 'w') as h5file:\n",
    "    for name, param in model.state_dict().items():\n",
    "        h5file.create_dataset(name, data=param.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load matlab saved file noisy_data.mat and print what variables are in it\n",
    "import scipy.io\n",
    "mat = scipy.io.loadmat('noisy_data.mat')\n",
    "print(mat.keys())\n",
    "# clean the double underscores from the variable names\n",
    "mat = {k.strip('_'): v for k, v in mat.items()}\n",
    "\n",
    "# load rest of the variables in mat into dictionary\n",
    "mat = {k: v for k, v in mat.items() if not k.startswith('__')}\n",
    "\n",
    "# create torch dataloader on the noisy data its synth_x_best as the output and synth_y_test as the input\n",
    "input_data_noisy = torch.tensor(mat['synth_y_test'], dtype=torch.float32)\n",
    "output_data_noisy = torch.tensor(mat['synth_x_best'], dtype=torch.float32)\n",
    "dataloader = DataLoader(TensorDataset(input_data_noisy, output_data_noisy), batch_size=100000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_input, batch_output in dataloader:\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(batch_input, batch_output, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Convert data to PyTorch tensors and send to the device (GPU if available)\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Define the loss function and the optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0000001)  # Decrease the learning rate\n",
    "\n",
    "    # Add a learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=1000, factor=0.5)\n",
    "\n",
    "    # Assuming all previous setup code remains the same and tensors (X_train_tensor, y_train_tensor, X_test_tensor)\n",
    "    # are already transferred to the appropriate device (GPU or CPU)\n",
    "\n",
    "    # No need for DataLoader as we will not use mini-batches\n",
    "\n",
    "    # Train the model without using batches\n",
    "    num_epochs = 1000\n",
    "    best_loss = np.inf\n",
    "    patience, trials = 2000, 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Ensure the entire dataset is on the same device as the model\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Adjust the learning rate based on the loss\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            outputs = model(X_test_tensor)\n",
    "            loss2 = criterion(outputs, y_test_tensor)\n",
    "            print('Epoch [{}/{}], Loss: {:.10f}, test Loss reached {:.10f}, min Loss reached {:.10f}, stagnating for {} it.'.format(epoch +\n",
    "                1, num_epochs, loss.item(),loss2.item(), best_loss, trials))\n",
    "            if loss2.item() > 10*best_loss:\n",
    "                print(f\"Overfitting detected at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # Early stopping\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            trials = 0\n",
    "        else:\n",
    "            trials += 1\n",
    "            if trials >= patience:\n",
    "                print(f'Stopping early at epoch {epoch+1}')\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
