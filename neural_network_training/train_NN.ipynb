{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook shows how to train a robust inverse of kamera operator\n",
    "\n",
    "- should accept vector of size 10 of 2D points in camera view (in meters)\n",
    "- should output a vector of size 6 of camera pose (3D position and 3D rotation)\n",
    "- should be robust to noise in input\n",
    "    - it is first trained as a pure inverse of kamera operator (pretrained available in `hot_start.h5`)\n",
    "    - then it is trained on a dataset of 800k samples obtained as follows:\n",
    "        - generate position and rotation of camera in feasible domain\n",
    "        - use kamera operator to generate 2D points\n",
    "        - add noise to 2D points\n",
    "        - use minimization in sense of least squares to find camera pose wit least error (Newton's method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from kamera import kamera  # kamera operator written in Python, equivalent to the one in Matlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feasible bounds of camera position and orientation\n",
    "bounds = np.array([50 * 1e-3, 50 * 1e-3, 50 * 1e-3, 20 * np.pi / 180, 20 * np.pi / 180, 20 * np.pi / 180])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network architecture\n",
    "\n",
    "- Perceptron with 3 hidden layers\n",
    "- activation function: Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distances_angles_efficient(points):\n",
    "    n = points.shape[0]  # Number of rows in the input\n",
    "    reshaped_points = points.reshape(n, 5, 2)  # Reshape to n x 5 x 2 for point coordinates\n",
    "\n",
    "    # Precompute all distances between points\n",
    "    def dist(p1, p2): return np.sqrt((p1[:, 0] - p2[:, 0])**2 + (p1[:, 1] - p2[:, 1])**2)\n",
    "    distances = np.hstack([\n",
    "        dist(reshaped_points[:, 0], reshaped_points[:, 1]).reshape(-1, 1),  # Distance 1-2\n",
    "        dist(reshaped_points[:, 1], reshaped_points[:, 2]).reshape(-1, 1),  # Distance 2-3\n",
    "        dist(reshaped_points[:, 2], reshaped_points[:, 0]).reshape(-1, 1),  # Distance 3-1\n",
    "        dist(reshaped_points[:, 0], reshaped_points[:, 4]).reshape(-1, 1),  # Distance 1-5\n",
    "        dist(reshaped_points[:, 4], reshaped_points[:, 1]).reshape(-1, 1),  # Distance 5-2\n",
    "        dist(reshaped_points[:, 0], reshaped_points[:, 3]).reshape(-1, 1),  # Distance 1-4\n",
    "        dist(reshaped_points[:, 4], reshaped_points[:, 3]).reshape(-1, 1),  # Distance 5-4\n",
    "        dist(reshaped_points[:, 3], reshaped_points[:, 2]).reshape(-1, 1)   # Distance 4-3\n",
    "    ])\n",
    "\n",
    "    # Using the law of cosines to compute angles for each triangle\n",
    "    def law_of_cosines(a, b, c): return np.arccos((a**2 + b**2 - c**2) / (2 * a * b))\n",
    "\n",
    "    angles = np.hstack([\n",
    "        law_of_cosines(distances[:, 0], distances[:, 2], distances[:, 1]).reshape(-1, 1),  # Angle 1 in triangle 1-2-3\n",
    "        law_of_cosines(distances[:, 0], distances[:, 1], distances[:, 2]).reshape(-1, 1),  # Angle 2 in triangle 1-2-3\n",
    "        law_of_cosines(distances[:, 1], distances[:, 2], distances[:, 0]).reshape(-1, 1),  # Angle 3 in triangle 1-2-3\n",
    "        law_of_cosines(distances[:, 0], distances[:, 3], distances[:, 4]).reshape(-1, 1),  # Angle 1 in triangle 1-2-5\n",
    "        law_of_cosines(distances[:, 0], distances[:, 4], distances[:, 3]).reshape(-1, 1),  # Angle 2 in triangle 1-2-5\n",
    "        law_of_cosines(distances[:, 3], distances[:, 4], distances[:, 0]).reshape(-1, 1),  # Angle 5 in triangle 1-2-5\n",
    "        law_of_cosines(distances[:, 3], distances[:, 5], distances[:, 6]).reshape(-1, 1),  # Angle 1 in triangle 1-5-4\n",
    "        law_of_cosines(distances[:, 3], distances[:, 6], distances[:, 5]).reshape(-1, 1),  # Angle 5 in triangle 1-5-4\n",
    "        law_of_cosines(distances[:, 5], distances[:, 6], distances[:, 3]).reshape(-1, 1),  # Angle 4 in triangle 1-5-4\n",
    "        law_of_cosines(distances[:, 5], distances[:, 2], distances[:, 7]).reshape(-1, 1),  # Angle 1 in triangle 1-4-3\n",
    "        law_of_cosines(distances[:, 5], distances[:, 7], distances[:, 2]).reshape(-1, 1),  # Angle 4 in triangle 1-4-3\n",
    "        law_of_cosines(distances[:, 2], distances[:, 7], distances[:, 5]).reshape(-1, 1)   # Angle 3 in triangle 1-4-3\n",
    "    ])\n",
    "\n",
    "    output = np.hstack([distances, angles])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, output_size, bounds, hidden_size=512):\n",
    "        super(Net, self).__init__()\n",
    "        self.bounds = torch.tensor(bounds, dtype=torch.float32,device='cuda')  # Convert bounds to a PyTorch tensor\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # self.fc1_ret = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4b = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc5 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc6 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc7 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc8 = nn.Linear(hidden_size, output_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.ReLu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tanh(self.fc1(x))\n",
    "        x = self.tanh(self.fc2(x))\n",
    "        x = self.tanh(self.fc3(x))\n",
    "        x = self.tanh(self.fc4(x))\n",
    "        x = self.tanh(self.fc4b(x))\n",
    "        x = self.tanh(self.fc5(x))\n",
    "        x = self.tanh(self.fc6(x))\n",
    "        x = self.tanh(self.fc7(x))\n",
    "        x = self.fc8(x)\n",
    "        x = x * self.bounds  # Scale the output by the bounds\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = Net(30, 6, bounds, hidden_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: load pretrained model\n",
    "- pretrained model is available in `hot_start.h5` file\n",
    "- its trained for around 24hours on 4070Rtx GPU\n",
    "- it is trained on synthetic data generated from feasible domain of camera pose and 2D points obtained from kamera operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Load the model weights from HDF5 file\n",
    "with h5py.File('trained_model_weights_9.h5', 'r') as h5file:\n",
    "    for name, param in model.named_parameters():\n",
    "        # Ensure the parameter name matches the HDF5 dataset name structure\n",
    "        param.data.copy_(torch.from_numpy(h5file[name][...]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move model to GPU (if available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=30, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc3): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc4): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc4b): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc5): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc6): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc7): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc8): Linear(in_features=512, out_features=6, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (ReLu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on synthetic data as precise inverse of kamera operator\n",
    "\n",
    "Training in batches of `n` samples:\n",
    "- generate random feasible camera pose and corresponding 2D points using kamera operator\n",
    "- split to train and test set\n",
    "- using Adam optimizer (you can adjust learning rate)\n",
    "- there is fixed number of epochs with some checks for early stopping (overfitting, and patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and the optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.LBFGS(model.parameters(), lr=0.5, line_search_fn='strong_wolfe') # Use the L-BFGS optimizer\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.0000001)  # Decrease the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial set 0:\n",
      "Epoch [1/2], Loss: 2.27187e+00, test Loss reached 1.71994e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.87367e+00, test Loss reached 1.63169e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 1:\n",
      "Epoch [1/2], Loss: 2.17026e+00, test Loss reached 1.66116e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.81175e+00, test Loss reached 1.61149e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 2:\n",
      "Epoch [1/2], Loss: 1.36183e+00, test Loss reached 1.30123e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.33038e+00, test Loss reached 1.29181e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 3:\n",
      "Epoch [1/2], Loss: 1.95669e+00, test Loss reached 1.53551e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.79827e+00, test Loss reached 1.52258e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 4:\n",
      "Epoch [1/2], Loss: 1.44972e+00, test Loss reached 8.92147e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.38714e+00, test Loss reached 9.35792e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 5:\n",
      "Epoch [1/2], Loss: 1.37846e+00, test Loss reached 1.32600e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.35167e+00, test Loss reached 1.32807e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 6:\n",
      "Epoch [1/2], Loss: 1.65196e+00, test Loss reached 1.38920e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.46834e+00, test Loss reached 1.37429e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 7:\n",
      "Epoch [1/2], Loss: 1.22889e+00, test Loss reached 1.38160e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.20782e+00, test Loss reached 1.35324e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 8:\n",
      "Epoch [1/2], Loss: 1.21367e+00, test Loss reached 1.83969e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.19199e+00, test Loss reached 1.78720e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 9:\n",
      "Epoch [1/2], Loss: 1.35488e+00, test Loss reached 1.67987e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.31082e+00, test Loss reached 1.58591e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 10:\n",
      "Epoch [1/2], Loss: 1.49440e+00, test Loss reached 1.48618e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.45622e+00, test Loss reached 1.46579e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 11:\n",
      "Epoch [1/2], Loss: 1.49884e+00, test Loss reached 1.40098e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.46861e+00, test Loss reached 1.39985e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 12:\n",
      "Epoch [1/2], Loss: 1.30432e+00, test Loss reached 1.35478e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.27413e+00, test Loss reached 1.32419e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 13:\n",
      "Epoch [1/2], Loss: 1.81992e+00, test Loss reached 1.22274e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.70653e+00, test Loss reached 1.19479e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 14:\n",
      "Epoch [1/2], Loss: 1.40252e+00, test Loss reached 1.09663e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.35839e+00, test Loss reached 1.06117e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 15:\n",
      "Epoch [1/2], Loss: 1.33053e+00, test Loss reached 1.13860e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.30867e+00, test Loss reached 1.12486e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 16:\n",
      "Epoch [1/2], Loss: 1.37870e+00, test Loss reached 1.05979e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.34753e+00, test Loss reached 1.06028e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 17:\n",
      "Epoch [1/2], Loss: 1.52163e+00, test Loss reached 1.18911e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.47789e+00, test Loss reached 1.20866e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 18:\n",
      "Epoch [1/2], Loss: 1.40345e+00, test Loss reached 1.46080e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.37531e+00, test Loss reached 1.48511e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 19:\n",
      "Epoch [1/2], Loss: 1.34800e+00, test Loss reached 1.24212e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.31937e+00, test Loss reached 1.24611e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 20:\n",
      "Epoch [1/2], Loss: 1.23272e+00, test Loss reached 2.37983e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.20915e+00, test Loss reached 2.20813e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 21:\n",
      "Epoch [1/2], Loss: 1.18975e+00, test Loss reached 1.64347e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.17347e+00, test Loss reached 1.66984e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 22:\n",
      "Epoch [1/2], Loss: 1.69911e+00, test Loss reached 1.50583e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.65414e+00, test Loss reached 1.48901e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 23:\n",
      "Epoch [1/2], Loss: 1.34024e+00, test Loss reached 1.53269e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.31506e+00, test Loss reached 1.52008e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 24:\n",
      "Epoch [1/2], Loss: 1.31922e+00, test Loss reached 1.31454e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.29704e+00, test Loss reached 1.33787e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 25:\n",
      "Epoch [1/2], Loss: 1.22051e+00, test Loss reached 1.10966e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.20337e+00, test Loss reached 1.11466e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 26:\n",
      "Epoch [1/2], Loss: 1.18151e+00, test Loss reached 1.10367e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.17170e+00, test Loss reached 1.10019e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 27:\n",
      "Epoch [1/2], Loss: 1.44817e+00, test Loss reached 1.16653e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.40676e+00, test Loss reached 1.16663e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 28:\n",
      "Epoch [1/2], Loss: 1.34932e+00, test Loss reached 1.50022e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.30153e+00, test Loss reached 1.53855e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 29:\n",
      "Epoch [1/2], Loss: 1.41245e+00, test Loss reached 1.45718e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.36714e+00, test Loss reached 1.42271e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 30:\n",
      "Epoch [1/2], Loss: 1.34866e+00, test Loss reached 1.29874e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.32013e+00, test Loss reached 1.27963e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 31:\n",
      "Epoch [1/2], Loss: 1.22926e+00, test Loss reached 1.74512e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.21456e+00, test Loss reached 1.74957e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 32:\n",
      "Epoch [1/2], Loss: 1.31845e+00, test Loss reached 1.11941e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.29307e+00, test Loss reached 1.12967e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 33:\n",
      "Epoch [1/2], Loss: 1.34083e+00, test Loss reached 1.11796e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.30072e+00, test Loss reached 1.09943e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 34:\n",
      "Epoch [1/2], Loss: 1.33423e+00, test Loss reached 1.33568e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.28761e+00, test Loss reached 1.32772e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 35:\n",
      "Epoch [1/2], Loss: 1.24762e+00, test Loss reached 1.46974e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.23390e+00, test Loss reached 1.47943e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 36:\n",
      "Epoch [1/2], Loss: 1.34695e+00, test Loss reached 1.17122e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.31007e+00, test Loss reached 1.17481e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 37:\n",
      "Epoch [1/2], Loss: 1.18160e+00, test Loss reached 1.22536e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.16933e+00, test Loss reached 1.20443e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 38:\n",
      "Epoch [1/2], Loss: 1.21766e+00, test Loss reached 1.23001e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.20802e+00, test Loss reached 1.23439e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 39:\n",
      "Epoch [1/2], Loss: 1.18265e+00, test Loss reached 1.09123e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.17264e+00, test Loss reached 1.10829e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 40:\n",
      "Epoch [1/2], Loss: 1.28096e+00, test Loss reached 1.22782e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.26489e+00, test Loss reached 1.23819e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 41:\n",
      "Epoch [1/2], Loss: 1.67023e+00, test Loss reached 1.57361e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.58939e+00, test Loss reached 1.54707e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 42:\n",
      "Epoch [1/2], Loss: 1.29697e+00, test Loss reached 2.57418e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.26760e+00, test Loss reached 2.69571e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 43:\n",
      "Epoch [1/2], Loss: 1.91391e+00, test Loss reached 1.47730e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.76630e+00, test Loss reached 1.39843e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 44:\n",
      "Epoch [1/2], Loss: 1.27711e+00, test Loss reached 1.26730e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.24835e+00, test Loss reached 1.27021e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 45:\n",
      "Epoch [1/2], Loss: 1.24662e+00, test Loss reached 1.45719e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.23329e+00, test Loss reached 1.48984e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 46:\n",
      "Epoch [1/2], Loss: 1.82134e+00, test Loss reached 1.59201e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.69685e+00, test Loss reached 1.55688e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 47:\n",
      "Epoch [1/2], Loss: 1.26972e+00, test Loss reached 5.64506e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.24836e+00, test Loss reached 5.56440e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 48:\n",
      "Epoch [1/2], Loss: 1.16739e+00, test Loss reached 1.24498e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.15519e+00, test Loss reached 1.23438e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 49:\n",
      "Epoch [1/2], Loss: 1.24457e+00, test Loss reached 1.56806e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.23216e+00, test Loss reached 1.56133e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 50:\n",
      "Epoch [1/2], Loss: 1.35058e+00, test Loss reached 1.21492e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.29303e+00, test Loss reached 1.23959e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 51:\n",
      "Epoch [1/2], Loss: 1.63566e+00, test Loss reached 1.99837e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.54112e+00, test Loss reached 1.91606e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 52:\n",
      "Epoch [1/2], Loss: 1.38977e+00, test Loss reached 3.77525e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.35862e+00, test Loss reached 4.16964e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 53:\n",
      "Epoch [1/2], Loss: 1.42152e+00, test Loss reached 4.87161e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.35896e+00, test Loss reached 4.57735e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 54:\n",
      "Epoch [1/2], Loss: 1.18068e+00, test Loss reached 1.19218e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.16825e+00, test Loss reached 1.18077e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 55:\n",
      "Epoch [1/2], Loss: 1.42621e+00, test Loss reached 1.54002e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.40166e+00, test Loss reached 1.51095e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 56:\n",
      "Epoch [1/2], Loss: 1.23943e+00, test Loss reached 1.10179e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.22104e+00, test Loss reached 1.12034e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 57:\n",
      "Epoch [1/2], Loss: 1.25435e+00, test Loss reached 1.46573e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.23326e+00, test Loss reached 1.47750e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 58:\n",
      "Epoch [1/2], Loss: 1.18958e+00, test Loss reached 2.65462e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.17676e+00, test Loss reached 2.43764e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 59:\n",
      "Epoch [1/2], Loss: 1.29020e+00, test Loss reached 1.03948e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.26811e+00, test Loss reached 1.03016e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 60:\n",
      "Epoch [1/2], Loss: 1.25332e+00, test Loss reached 1.72655e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.23751e+00, test Loss reached 1.73891e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 61:\n",
      "Epoch [1/2], Loss: 1.15324e+00, test Loss reached 1.31199e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.13827e+00, test Loss reached 1.30520e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 62:\n",
      "Epoch [1/2], Loss: 1.23931e+00, test Loss reached 1.11059e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.21981e+00, test Loss reached 1.11359e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 63:\n",
      "Epoch [1/2], Loss: 1.20544e+00, test Loss reached 1.04366e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.19333e+00, test Loss reached 1.05919e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 64:\n",
      "Epoch [1/2], Loss: 1.34800e+00, test Loss reached 4.44300e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.30071e+00, test Loss reached 4.43125e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 65:\n",
      "Epoch [1/2], Loss: 1.19182e+00, test Loss reached 1.24896e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.17801e+00, test Loss reached 1.24597e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 66:\n",
      "Epoch [1/2], Loss: 1.31897e+00, test Loss reached 1.15226e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.29671e+00, test Loss reached 1.16933e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 67:\n",
      "Epoch [1/2], Loss: 1.41750e+00, test Loss reached 1.37120e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.36955e+00, test Loss reached 1.34990e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 68:\n",
      "Epoch [1/2], Loss: 1.27509e+00, test Loss reached 1.20688e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.26099e+00, test Loss reached 1.21525e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 69:\n",
      "Epoch [1/2], Loss: 1.39597e+00, test Loss reached 1.15928e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.34607e+00, test Loss reached 1.18873e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 70:\n",
      "Epoch [1/2], Loss: 1.18536e+00, test Loss reached 1.11917e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.17526e+00, test Loss reached 1.12193e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 71:\n",
      "Epoch [1/2], Loss: 1.19804e+00, test Loss reached 1.95624e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.18503e+00, test Loss reached 1.94414e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 72:\n",
      "Epoch [1/2], Loss: 1.15332e+00, test Loss reached 1.22476e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.14008e+00, test Loss reached 1.22970e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 73:\n",
      "Epoch [1/2], Loss: 1.18891e+00, test Loss reached 7.02535e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.17773e+00, test Loss reached 7.15120e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 74:\n",
      "Epoch [1/2], Loss: 1.16178e+00, test Loss reached 1.37639e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.15165e+00, test Loss reached 1.37220e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 75:\n",
      "Epoch [1/2], Loss: 1.27164e+00, test Loss reached 1.39348e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.25711e+00, test Loss reached 1.41167e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 76:\n",
      "Epoch [1/2], Loss: 1.21447e+00, test Loss reached 1.25096e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.19805e+00, test Loss reached 1.26800e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 77:\n",
      "Epoch [1/2], Loss: 1.24663e+00, test Loss reached 1.12863e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.22359e+00, test Loss reached 1.13613e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 78:\n",
      "Epoch [1/2], Loss: 1.11050e+00, test Loss reached 1.33508e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.09966e+00, test Loss reached 1.35521e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 79:\n",
      "Epoch [1/2], Loss: 1.24101e+00, test Loss reached 1.11176e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Epoch [2/2], Loss: 1.22221e+00, test Loss reached 1.10458e+00, min Loss reached inf, stagnating for 0 it.\n",
      "Trial set 80:\n",
      "Epoch [1/2], Loss: 1.15810e+00, test Loss reached 1.54845e+00, min Loss reached inf, stagnating for 0 it.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 50\u001b[0m\n\u001b[1;32m     40\u001b[0m patience, trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# outputs = model(X_train_tensor)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# optimizer.zero_grad()\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# loss.backward()\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# Adjust the learning rate based on the loss\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# scheduler.step(loss)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# Print the loss every 100 epochs\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/optim/lbfgs.py:428\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(x, t, d):\n\u001b[1;32m    426\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_directional_evaluate(closure, x, t, d)\n\u001b[0;32m--> 428\u001b[0m     loss, flat_grad, t, ls_func_evals \u001b[38;5;241m=\u001b[39m \u001b[43m_strong_wolfe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgtd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_grad(t, d)\n\u001b[1;32m    431\u001b[0m opt_cond \u001b[38;5;241m=\u001b[39m flat_grad\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tolerance_grad\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/optim/lbfgs.py:50\u001b[0m, in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m     48\u001b[0m g \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mclone(memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcontiguous_format)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# evaluate objective and gradient using initial step\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m f_new, g_new \u001b[38;5;241m=\u001b[39m \u001b[43mobj_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m ls_func_evals \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     52\u001b[0m gtd_new \u001b[38;5;241m=\u001b[39m g_new\u001b[38;5;241m.\u001b[39mdot(d)\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/optim/lbfgs.py:426\u001b[0m, in \u001b[0;36mLBFGS.step.<locals>.obj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(x, t, d):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_directional_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/optim/lbfgs.py:280\u001b[0m, in \u001b[0;36mLBFGS._directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_directional_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure, x, t, d):\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_grad(t, d)\n\u001b[0;32m--> 280\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(closure())\n\u001b[1;32m    281\u001b[0m     flat_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gather_flat_grad()\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_param(x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# def criterion(outputs, y_train_tensor):\n",
    "#     vals = torch.sum((outputs - y_train_tensor)**6,dim=1)*1e20\n",
    "#     mask = vals > 0.00001\n",
    "#     return torch.mean(vals[mask])\n",
    "\n",
    "def criterion(outputs, y_train_tensor):\n",
    "    vals = torch.sum((outputs - y_train_tensor)**2, dim=1)**3 * 1e22\n",
    "    return torch.mean(vals)\n",
    "\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "for trial_sets in range(1000000):\n",
    "    print(f\"Trial set {trial_sets}:\")\n",
    "\n",
    "    n = int(1e5)  # number of samples\n",
    "    output_data = bounds * (np.random.rand(n, 6) - 0.5) * 2  # generate random camera positions and orientations\n",
    "    input_data1 = kamera(output_data)  # calculate the corresponding points positions in the image\n",
    "    input_data2 = compute_distances_angles_efficient(input_data1)\n",
    "    input_data = np.hstack([input_data1, input_data2])\n",
    "\n",
    "    # divide the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(input_data, output_data, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Convert data to PyTorch tensors and send to the device (GPU if available)\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Train the model without using batches\n",
    "    num_epochs = 2\n",
    "    best_loss = np.inf\n",
    "    patience, trials = 2000, 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass\n",
    "        # outputs = model(X_train_tensor)\n",
    "        # loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "        # # Backward and optimize\n",
    "        # optimizer.zero_grad()\n",
    "        # loss.backward()\n",
    "        optimizer.step(closure=closure)\n",
    "\n",
    "        # Adjust the learning rate based on the loss\n",
    "        # scheduler.step(loss)\n",
    "\n",
    "        # Print the loss every 100 epochs\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(X_test_tensor)\n",
    "                loss2 = criterion(outputs, y_test_tensor)\n",
    "                outputs = model(X_train_tensor)\n",
    "                loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "            print('Epoch [{}/{}], Loss: {:.5e}, test Loss reached {:.5e}, min Loss reached {:.5e}, stagnating for {} it.'.format(epoch +\n",
    "                                                                                                                                    1, num_epochs, loss.item(), loss2.item(), best_loss, trials))\n",
    "            if loss2.item() > 10 * best_loss:\n",
    "                print(f\"Overfitting detected at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # Early stopping\n",
    "        # if loss.item() < best_loss:\n",
    "        #     best_loss = loss.item()\n",
    "        #     trials = 0\n",
    "        # else:\n",
    "        #     trials += 1\n",
    "        #     if trials >= patience:\n",
    "        #         print(f'Stopping early at epoch {epoch+1}')\n",
    "        #         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#s of the fc1 layer\n",
    "# Since the model is on GPU, the weights will be on GPU as well\n",
    "# We need to move them to CPU and convert to numpy for plotting\n",
    "weights = model.fc1.weight.data.cpu().numpy()\n",
    "\n",
    "# Plot the weights matrix using imshow\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(weights, aspect='auto', cmap='viridis')  # You can choose any colormap you like\n",
    "plt.colorbar()\n",
    "plt.title('Weights of fc1 Layer')\n",
    "plt.xlabel('Output Neurons')\n",
    "plt.ylabel('Input Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.89 GiB. GPU 0 has a total capacity of 10.90 GiB of which 850.25 MiB is free. Including non-PyTorch memory, this process has 9.18 GiB memory in use. Of the allocated memory 6.99 GiB is allocated by PyTorch, and 2.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m y_test_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y_test, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 17\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     loss2 \u001b[38;5;241m=\u001b[39m criterion(outputs, y_test_tensor)\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 20\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[0;32m---> 20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x))\n\u001b[1;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc4(x))\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/activation.py:356\u001b[0m, in \u001b[0;36mTanh.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.89 GiB. GPU 0 has a total capacity of 10.90 GiB of which 850.25 MiB is free. Including non-PyTorch memory, this process has 9.18 GiB memory in use. Of the allocated memory 6.99 GiB is allocated by PyTorch, and 2.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "n = int(1e6)  # number of samples\n",
    "output_data = bounds * (np.random.rand(n, 6) - 0.5) * 2  # generate random camera positions and orientations\n",
    "input_data1 = kamera(output_data)  # calculate the corresponding points positions in the image\n",
    "input_data2 = compute_distances_angles_efficient(input_data1)\n",
    "input_data = np.hstack([input_data1, input_data2])\n",
    "\n",
    "# divide the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_data, output_data, test_size=0.99, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors and send to the device (GPU if available)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    loss2 = criterion(outputs, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0011111111111111111"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(np.log10(np.linalg.norm(X_test_tensor[:,:10].detach().cpu().numpy() - kamera(outputs.detach().cpu().numpy()),axis=1)**2), bins=1000);\n",
    "np.sum(\n",
    "    np.log10(\n",
    "        np.linalg.norm(\n",
    "            X_test_tensor[:, :10].detach().cpu().numpy() -\n",
    "            kamera(\n",
    "                outputs.detach().cpu().numpy()),\n",
    "            axis=1)**2)>-3)/(n*0.99)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.log10(\n",
    "    np.linalg.norm(\n",
    "        X_test_tensor[:, :10].detach().cpu().numpy() -\n",
    "        kamera(\n",
    "            outputs.detach().cpu().numpy()),\n",
    "        axis=1)**2)>-4\n",
    "(torch.sum((outputs[mask, :] - y_test_tensor[mask,:])**6,dim=1 )* 1e20).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(\n",
    "    np.log10(\n",
    "        np.linalg.norm(\n",
    "            X_test_tensor[:, :10].detach().cpu().numpy() -\n",
    "            kamera(\n",
    "                outputs.detach().cpu().numpy()),\n",
    "            axis=1)**2) >-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distances_angles_efficient(points):\n",
    "    n = points.shape[0]  # Number of rows in the input\n",
    "    reshaped_points = points.reshape(n, 5, 2)  # Reshape to n x 5 x 2 for point coordinates\n",
    "\n",
    "    # Precompute all distances between points\n",
    "    def dist(p1, p2): return np.sqrt((p1[:, 0] - p2[:, 0])**2 + (p1[:, 1] - p2[:, 1])**2)\n",
    "    distances = np.hstack([\n",
    "        dist(reshaped_points[:, 0], reshaped_points[:, 1]).reshape(-1, 1),  # Distance 1-2\n",
    "        dist(reshaped_points[:, 1], reshaped_points[:, 2]).reshape(-1, 1),  # Distance 2-3\n",
    "        dist(reshaped_points[:, 2], reshaped_points[:, 0]).reshape(-1, 1),  # Distance 3-1\n",
    "        dist(reshaped_points[:, 0], reshaped_points[:, 4]).reshape(-1, 1),  # Distance 1-5\n",
    "        dist(reshaped_points[:, 4], reshaped_points[:, 1]).reshape(-1, 1),  # Distance 5-2\n",
    "        dist(reshaped_points[:, 0], reshaped_points[:, 3]).reshape(-1, 1),  # Distance 1-4\n",
    "        dist(reshaped_points[:, 4], reshaped_points[:, 3]).reshape(-1, 1),  # Distance 5-4\n",
    "        dist(reshaped_points[:, 3], reshaped_points[:, 2]).reshape(-1, 1)   # Distance 4-3\n",
    "    ])\n",
    "\n",
    "    # Using the law of cosines to compute angles for each triangle\n",
    "    def law_of_cosines(a, b, c): return np.arccos((a**2 + b**2 - c**2) / (2 * a * b))\n",
    "\n",
    "    angles = np.hstack([\n",
    "        law_of_cosines(distances[:, 0], distances[:, 2], distances[:, 1]).reshape(-1, 1),  # Angle 1 in triangle 1-2-3\n",
    "        law_of_cosines(distances[:, 0], distances[:, 1], distances[:, 2]).reshape(-1, 1),  # Angle 2 in triangle 1-2-3\n",
    "        law_of_cosines(distances[:, 1], distances[:, 2], distances[:, 0]).reshape(-1, 1),  # Angle 3 in triangle 1-2-3\n",
    "        law_of_cosines(distances[:, 0], distances[:, 3], distances[:, 4]).reshape(-1, 1),  # Angle 1 in triangle 1-2-5\n",
    "        law_of_cosines(distances[:, 0], distances[:, 4], distances[:, 3]).reshape(-1, 1),  # Angle 2 in triangle 1-2-5\n",
    "        law_of_cosines(distances[:, 3], distances[:, 4], distances[:, 0]).reshape(-1, 1),  # Angle 5 in triangle 1-2-5\n",
    "        law_of_cosines(distances[:, 3], distances[:, 5], distances[:, 6]).reshape(-1, 1),  # Angle 1 in triangle 1-5-4\n",
    "        law_of_cosines(distances[:, 3], distances[:, 6], distances[:, 5]).reshape(-1, 1),  # Angle 5 in triangle 1-5-4\n",
    "        law_of_cosines(distances[:, 5], distances[:, 6], distances[:, 3]).reshape(-1, 1),  # Angle 4 in triangle 1-5-4\n",
    "        law_of_cosines(distances[:, 5], distances[:, 2], distances[:, 7]).reshape(-1, 1),  # Angle 1 in triangle 1-4-3\n",
    "        law_of_cosines(distances[:, 5], distances[:, 7], distances[:, 2]).reshape(-1, 1),  # Angle 4 in triangle 1-4-3\n",
    "        law_of_cosines(distances[:, 2], distances[:, 7], distances[:, 5]).reshape(-1, 1)   # Angle 3 in triangle 1-4-3\n",
    "    ])\n",
    "\n",
    "    output = np.hstack([distances, angles])\n",
    "    return output\n",
    "\n",
    "\n",
    "def plot_points_with_enhanced_details(points, computed_results):\n",
    "    \"\"\"\n",
    "    Enhances the previous function to plot points with distances and angles annotated, including lines for each triangle\n",
    "    and shifting angle text towards the center of each triangle for better clarity.\n",
    "\n",
    "    Parameters:\n",
    "    - points: An array of points' positions.\n",
    "    - computed_results: An array of computed distances and angles.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Plot points and label them\n",
    "    for i, point in enumerate(points):\n",
    "        plt.plot(point[0], point[1], 'o', label=f'Point {i+1}')\n",
    "        plt.text(point[0], point[1], f' P{i+1}', verticalalignment='bottom', horizontalalignment='right', fontsize=9)\n",
    "\n",
    "    # Annotate distances\n",
    "    distances = computed_results[0, :8]  # First 8 are distances\n",
    "    distance_pairs = [(0, 1), (1, 2), (2, 0), (0, 4), (4, 1), (0, 3), (4, 3), (3, 2)]\n",
    "    for i, pair in enumerate(distance_pairs):\n",
    "        p1, p2 = points[pair[0]], points[pair[1]]\n",
    "        mid_point = (p1 + p2) / 2\n",
    "        plt.text(mid_point[0], mid_point[1], f'{distances[i]:.2f}', color='blue', fontsize=9, ha='center')\n",
    "\n",
    "    # Draw lines for each triangle and annotate angles\n",
    "    triangles = [(0, 1, 2), (0, 1, 4), (0, 4, 3), (0, 3, 2)]\n",
    "    angles = computed_results[0, 8:]  # Angles\n",
    "    angle_idx = 0  # Index to access angles array\n",
    "    for triangle in triangles:\n",
    "        # Calculate triangle centroid for angle text placement\n",
    "        centroid = np.mean(points[list(triangle)], axis=0)\n",
    "        for i in range(3):\n",
    "            # Draw lines\n",
    "            p1, p2 = points[triangle[i]], points[triangle[(i + 1) % 3]]\n",
    "            plt.plot([p1[0], p2[0]], [p1[1], p2[1]], 'g--', alpha=0.75)  # Triangle lines\n",
    "\n",
    "            # Annotate angles, slightly shifted towards centroid\n",
    "            angle_pos = points[triangle[i]] + (centroid - points[triangle[i]]) * 0.2\n",
    "            plt.text(angle_pos[0], angle_pos[1], f'{angles[angle_idx]:.3f} rad', color='red', fontsize=9, ha='center')\n",
    "            angle_idx += 1\n",
    "\n",
    "    plt.title('Enhanced Points with Distances and Angles')\n",
    "    plt.xlabel('X axis')\n",
    "    plt.ylabel('Y axis')\n",
    "    plt.axis('equal')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = compute_distances_angles_efficient(input_data)\n",
    "# Using the enhanced function with the previous points and computed results\n",
    "plot_points_with_enhanced_details(input_data[2, :].reshape(5, -1), results[2, :].reshape(1, -1))\n",
    "\n",
    "\n",
    "# Replicating the input to form an n x 10 matrix and test the corrected function\n",
    "\n",
    "results[0, :8], results[0, 8:]  # Displaying the first row's distances and angles as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on noisy data\n",
    "\n",
    "## Load noisy data (and their optimal camera pose as Least squares solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load matlab saved file noisy_data.mat and print what variables are in it\n",
    "import scipy.io\n",
    "mat = scipy.io.loadmat('noisy_data.mat')\n",
    "\n",
    "# create torch dataloader on the noisy data its synth_x_best as the output and synth_y_test as the input\n",
    "input_data_noisy = torch.tensor(mat['synth_y_test'], dtype=torch.float32)\n",
    "output_data_noisy = torch.tensor(mat['synth_x_best'], dtype=torch.float32)\n",
    "dataloader = DataLoader(TensorDataset(input_data_noisy, output_data_noisy), batch_size=10000, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on batches of `n` samples of noisy data\n",
    "\n",
    "- recomended to use smaller number of epochs and repeat whole proces multiple times with adjusted learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_input, batch_output in dataloader:\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(batch_input, batch_output, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Convert data to PyTorch tensors and send to the device (GPU if available)\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Define the loss function and the optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)  # Decrease the learning rate\n",
    "\n",
    "    # Add a learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=100, factor=0.5)\n",
    "\n",
    "    # Train the model without using batches\n",
    "    num_epochs = 100000\n",
    "    best_loss = np.inf\n",
    "    patience, trials = 5000, 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Ensure the entire dataset is on the same device as the model\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Adjust the learning rate based on the loss\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            outputs = model(X_test_tensor)\n",
    "            loss2 = criterion(outputs, y_test_tensor)\n",
    "            print('Epoch [{}/{}], Loss: {:.10f}, test Loss reached {:.10f}, min Loss reached {:.10f}, stagnating for {} it.'.format(epoch +\n",
    "                1, num_epochs, loss.item(),loss2.item(), best_loss, trials))\n",
    "            if loss2.item() > 10*best_loss:\n",
    "                print(f\"Overfitting detected at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # Early stopping\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            trials = 0\n",
    "        else:\n",
    "            trials += 1\n",
    "            if trials >= patience:\n",
    "                print(f'Stopping early at epoch {epoch+1}')\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the trained model to h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# If your model is on CUDA, move it back to CPU\n",
    "model.to('cpu')\n",
    "\n",
    "# Save model parameters to HDF5, including biases\n",
    "with h5py.File('trained_model_weights_10.h5', 'w') as h5file:\n",
    "    for name, param in model.state_dict().items():\n",
    "        h5file.create_dataset(name, data=param.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
