{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook shows how to train a robust inverse of kamera operator\n",
    "\n",
    "- should accept vector of size 10 of 2D points in camera view (in meters)\n",
    "- should output a vector of size 6 of camera pose (3D position and 3D rotation)\n",
    "- should be robust to noise in input\n",
    "    - it is first trained as a pure inverse of kamera operator (pretrained available in `hot_start.h5`)\n",
    "    - then it is trained on a dataset of 800k samples obtained as follows:\n",
    "        - generate position and rotation of camera in feasible domain\n",
    "        - use kamera operator to generate 2D points\n",
    "        - add noise to 2D points\n",
    "        - use minimization in sense of least squares to find camera pose wit least error (Newton's method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from kamera import kamera  # kamera operator written in Python, equivalent to the one in Matlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feasible bounds of camera position and orientation\n",
    "bounds = np.array([50 * 1e-3, 50 * 1e-3, 50 * 1e-3, 20 * np.pi / 180, 20 * np.pi / 180, 20 * np.pi / 180])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network architecture\n",
    "\n",
    "- Perceptron with 3 hidden layers\n",
    "- activation function: Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 512)\n",
    "        self.fc3 = nn.Linear(512, 512)\n",
    "        self.fc4 = nn.Linear(512, 512)\n",
    "        self.fc8 = nn.Linear(512, output_size)\n",
    "        self.relu = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.fc8(x)\n",
    "        return x\n",
    "\n",
    "model = Net(10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: load pretrained model\n",
    "- pretrained model is available in `hot_start.h5` file\n",
    "- its trained for around 24hours on 4070Rtx GPU\n",
    "- it is trained on synthetic data generated from feasible domain of camera pose and 2D points obtained from kamera operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Load the model weights from HDF5 file\n",
    "with h5py.File('hot_start.h5', 'r') as h5file:\n",
    "    for name, param in model.named_parameters():\n",
    "        # Ensure the parameter name matches the HDF5 dataset name structure\n",
    "        param.data.copy_(torch.from_numpy(h5file[name][...]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move model to GPU (if available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on synthetic data as precise inverse of kamera operator\n",
    "\n",
    "Training in batches of `n` samples:\n",
    "- generate random feasible camera pose and corresponding 2D points using kamera operator\n",
    "- split to train and test set\n",
    "- using Adam optimizer (you can adjust learning rate)\n",
    "- there is fixed number of epochs with some checks for early stopping (overfitting, and patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial_sets in range(100000):\n",
    "    print(f\"Trial set {trial_sets}:\")\n",
    "    \n",
    "    n = int(1e4)  # number of samples\n",
    "    output_data = bounds * (np.random.rand(n, 6) - 0.5) * 2  # generate random camera positions and orientations\n",
    "    input_data = kamera(output_data)  # calculate the corresponding points positions in the image\n",
    "    \n",
    "    # divide the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(input_data, output_data, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Convert data to PyTorch tensors and send to the device (GPU if available)\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Define the loss function and the optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)  # Decrease the learning rate\n",
    "\n",
    "    # Add a learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=100, factor=0.5)\n",
    "\n",
    "    # Train the model without using batches\n",
    "    num_epochs = 10000\n",
    "    best_loss = np.inf\n",
    "    patience, trials = 2000, 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Adjust the learning rate based on the loss\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        # Print the loss every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            outputs = model(X_test_tensor)\n",
    "            loss2 = criterion(outputs, y_test_tensor)\n",
    "            print('Epoch [{}/{}], Loss: {:.10f}, test Loss reached {:.10f}, min Loss reached {:.10f}, stagnating for {} it.'.format(epoch +\n",
    "                1, num_epochs, loss.item(),loss2.item(), best_loss, trials))\n",
    "            if loss2.item() > 10*best_loss:\n",
    "                print(f\"Overfitting detected at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # Early stopping\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            trials = 0\n",
    "        else:\n",
    "            trials += 1\n",
    "            if trials >= patience:\n",
    "                print(f'Stopping early at epoch {epoch+1}')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on noisy data\n",
    "\n",
    "## Load noisy data (and their optimal camera pose as Least squares solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load matlab saved file noisy_data.mat and print what variables are in it\n",
    "import scipy.io\n",
    "mat = scipy.io.loadmat('noisy_data.mat')\n",
    "\n",
    "# create torch dataloader on the noisy data its synth_x_best as the output and synth_y_test as the input\n",
    "input_data_noisy = torch.tensor(mat['synth_y_test'], dtype=torch.float32)\n",
    "output_data_noisy = torch.tensor(mat['synth_x_best'], dtype=torch.float32)\n",
    "dataloader = DataLoader(TensorDataset(input_data_noisy, output_data_noisy), batch_size=10000, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on batches of `n` samples of noisy data\n",
    "\n",
    "- recomended to use smaller number of epochs and repeat whole proces multiple times with adjusted learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_input, batch_output in dataloader:\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(batch_input, batch_output, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Convert data to PyTorch tensors and send to the device (GPU if available)\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Define the loss function and the optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)  # Decrease the learning rate\n",
    "\n",
    "    # Add a learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=100, factor=0.5)\n",
    "\n",
    "    # Train the model without using batches\n",
    "    num_epochs = 100000\n",
    "    best_loss = np.inf\n",
    "    patience, trials = 5000, 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Ensure the entire dataset is on the same device as the model\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Adjust the learning rate based on the loss\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            outputs = model(X_test_tensor)\n",
    "            loss2 = criterion(outputs, y_test_tensor)\n",
    "            print('Epoch [{}/{}], Loss: {:.10f}, test Loss reached {:.10f}, min Loss reached {:.10f}, stagnating for {} it.'.format(epoch +\n",
    "                1, num_epochs, loss.item(),loss2.item(), best_loss, trials))\n",
    "            if loss2.item() > 10*best_loss:\n",
    "                print(f\"Overfitting detected at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # Early stopping\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            trials = 0\n",
    "        else:\n",
    "            trials += 1\n",
    "            if trials >= patience:\n",
    "                print(f'Stopping early at epoch {epoch+1}')\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the trained model to h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# If your model is on CUDA, move it back to CPU\n",
    "model.to('cpu')\n",
    "\n",
    "# Save model parameters to HDF5, including biases\n",
    "with h5py.File('trained_model_weights.h5', 'w') as h5file:\n",
    "    for name, param in model.state_dict().items():\n",
    "        h5file.create_dataset(name, data=param.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
